{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vXO5SC3yHwKC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.datasets import Multi30k, IWSLT2016\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torch.utils.data import DataLoader\n",
        "import math as m\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formula(Q, K, V, dim = 4):\n",
        "  QK = torch.matmul(Q, K.T)\n",
        "  matmul = QK / math.sqrt(dim)\n",
        "  weights = F.softmax(matmul, dim = -1)\n",
        "  out = torch.matmul(weights, V)\n",
        "  return out, weights"
      ],
      "metadata": {
        "id": "lfmJ49kDTlyN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(Q, K, V):\n",
        "  out, weights = formula(Q, K, V)\n",
        "  out, weights = out.numpy(), weights.numpy()\n",
        "  print(np.round(out, 4))\n",
        "  print(np.round(weights, 4))"
      ],
      "metadata": {
        "id": "nTnvGobfUEdq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, dim_model = 8, number_of_heads = 4, dropout = 0.2):\n",
        "    super().__init__()\n",
        "    self.d = dim_model // number_of_heads\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.Qs = nn.ModuleList([nn.Linear(dim_model, self.d) for _ in range(number_of_heads)])\n",
        "    self.Ks = nn.ModuleList([nn.Linear(dim_model, self.d) for _ in range(number_of_heads)])\n",
        "    self.Vs = nn.ModuleList([nn.Linear(dim_model, self.d) for _ in range(number_of_heads)])\n",
        "    self.mha_linear = nn.Linear(dim_model, dim_model)\n",
        "\n",
        "  def formula(self, Q, K, V):\n",
        "    QK = torch.matmul(Q, K.permute(0, 2, 1))\n",
        "    matmul = QK / math.sqrt(self.d)\n",
        "    weights = F.softmax(matmul, dim = -1)\n",
        "    out = torch.matmul(weights, V)\n",
        "    return out, weights\n",
        "\n",
        "  def forward(self, pre_q, pre_k, pre_v, mask = None):\n",
        "    # shape(x) = [B x seq_len x D]\n",
        "    Q = [Q(pre_q) for Q in self.Qs]\n",
        "    K = [K(pre_k) for K in self.Ks]\n",
        "    V = [V(pre_v) for V in self.Vs]\n",
        "    output_per_head = []\n",
        "    weights_per_head = []\n",
        "    for q, k, v in zip(Q, K, V):\n",
        "      output, weight = self.formula(q, k, v)\n",
        "      output_per_head.append(output)\n",
        "      weights_per_head.append(weight)\n",
        "\n",
        "    output = torch.cat(output_per_head, -1)\n",
        "    weights = torch.stack(weights_per_head).permute(1, 0, 2, 3)\n",
        "    x = self.dropout(self.mha_linaer(output))\n",
        "    return x, output\n"
      ],
      "metadata": {
        "id": "0Aj5WkFvLymu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualNorm(nn.Module):\n",
        "    def __init__(self, dim_model, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(dim_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "      return self.layer_norm(self.dropout(x) + residual)"
      ],
      "metadata": {
        "id": "nVuZJ-uHtECm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, dim_model, number_of_heads, d_ff, dropout = 0.2):\n",
        "    super().__init__()\n",
        "    self.norm_1 = ResidualNorm(dim_model, dropout)\n",
        "    self.norm_2 = ResidualNorm(dim_model, dropout)\n",
        "    self.mha = MultiHeadAttention(dim_model, number_of_heads)\n",
        "    self.ff = nn.Sequential(\n",
        "            nn.Linear(dim_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, dim_model)\n",
        "        )\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    mha, encoder_weights = self.mha(x, x, x, mask = mask)\n",
        "    norm1 = self.norm_1(mha, x)\n",
        "    ff = self.ff(norm1)\n",
        "    norm2 = self.norm_2(ff, norm1)\n",
        "    return norm2, encoder_weights"
      ],
      "metadata": {
        "id": "JdzeAV78MUcT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, dropout=0.2, max_seq_len = 200, device = \"cuda\"):\n",
        "        super().__init__()\n",
        "        self.dim_model = dim_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_seq_len, dim_model).to(device)\n",
        "        pos = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
        "        two_i = torch.arange(0, dim_model, step=2).float()\n",
        "        div_term = torch.pow(10000, (two_i / torch.Tensor([dim_model]))).float()\n",
        "        pe[:, 0::2] = torch.sin(pos/div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos/div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        one_batch_pe: torch.Tensor = self.pe[:, :x.shape[1]].detach()\n",
        "        repeated_pe = one_batch_pe.repeat([x.shape[0], 1, 1]).detach()\n",
        "        x = x.add(repeated_pe)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "KU--rcrUtDMK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, dim_model, num_heads, feed_forward_dim, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.norm_1 = ResidualNorm(dim_model)\n",
        "        self.norm_2 = ResidualNorm(dim_model)\n",
        "        self.norm_3 = ResidualNorm(dim_model)\n",
        "        self.masked_attention = MultiHeadAttention(dim_model, num_heads, dropout)\n",
        "        self.encoder_decoder_attention = MultiHeadAttention(dim_model, num_heads, dropout)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "          nn.Linear(dim_model, feed_forward_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(dropout),\n",
        "          nn.Linear(feed_forward_dim, dim_model))\n",
        "\n",
        "    def forward(self, input_tensor, encoder_outputs, target_mask, source_mask):\n",
        "      masked_attention_output, masked_attention_weights = self.masked_attention(input_tensor, input_tensor, input_tensor, mask=target_mask)\n",
        "      norm1 = self.norm_1(masked_attention_output, input_tensor)\n",
        "      encoder_decoder_attention_output, encoder_decoder_attention_weights = self.encoder_decoder_attention(norm1, encoder_outputs, encoder_outputs, mask=source_mask)\n",
        "      norm2 = self.norm_2(encoder_decoder_attention_output, norm1)\n",
        "      feed_forward_output = self.feed_forward(norm2)\n",
        "      norm3 = self.norm_3(feed_forward_output, norm2)\n",
        "      return norm3, masked_attention_weights, encoder_decoder_attention_weights"
      ],
      "metadata": {
        "id": "Y6OdfYR3Ynix"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, padding_idx, dim_model):\n",
        "        super().__init__()\n",
        "        self.d_model = dim_model\n",
        "        self.embed = nn.Embedding(vocab_size, dim_model, padding_idx=padding_idx)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.embed(x)\n",
        "        return embedding * math.sqrt(self.dim_model)"
      ],
      "metadata": {
        "id": "vaKwtN_7aDSc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding, model_dim, num_heads, num_layers, feed_forward_dim, device=\"cuda\", dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = embedding\n",
        "        self.positional_encoding = PositionalEncoding(model_dim, device=device)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(\n",
        "            model_dim,\n",
        "            num_heads,\n",
        "            feed_forward_dim,\n",
        "            dropout,\n",
        "        ) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, input_tensor, encoder_output, target_mask, source_mask):\n",
        "        embeddings = self.embedding(input_tensor)\n",
        "        encoding = self.positional_encoding(embeddings)\n",
        "        for decoder in self.decoder_layers:\n",
        "            encoding, masked_attention_weights, encoder_decoder_attention_weights = decoder(encoding, encoder_output, target_mask, source_mask)\n",
        "        return encoding, masked_attention_weights, encoder_decoder_attention_weights"
      ],
      "metadata": {
        "id": "rmrFfyoXZMfE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding, model_dim,\n",
        "                 num_heads, num_layers,\n",
        "                 feed_forward_dim, device=\"cuda\", dropout = 0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = embedding\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            model_dim, device=device)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(\n",
        "            model_dim,\n",
        "            num_heads,\n",
        "            feed_forward_dim,\n",
        "            dropout\n",
        "        ) for _ in range(num_layers)])\n",
        "    def forward(self, input_tensor, mask=None):\n",
        "        embeddings = self.embedding(input_tensor)\n",
        "        encoding = self.positional_encoding(embeddings)\n",
        "        for encoder in self.encoder_layers:\n",
        "            encoding, encoder_attention_weights = encoder(encoding, mask)\n",
        "        return encoding, encoder_attention_weights"
      ],
      "metadata": {
        "id": "fJZGOvf7aeGA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_len, trg_vocab_len, d_model, d_ff,\n",
        "                 num_layers, num_heads, src_pad_idx, trg_pad_idx, dropout=0.2, device=\"cuda\"):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.device = device\n",
        "        encoder_Embedding = Embeddings(src_vocab_len, src_pad_idx, d_model)\n",
        "        decoder_Embedding = Embeddings(trg_vocab_len, trg_pad_idx, d_model)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.encoder = Encoder(encoder_Embedding, d_model, num_heads, num_layers, d_ff, device, dropout)\n",
        "        self.decoder = Decoder(decoder_Embedding, d_model, num_heads, num_layers, d_ff, device, dropout)\n",
        "        self.linear_layer = nn.Linear(d_model, trg_vocab_len)\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1)\n",
        "        if self.efficient_mha:\n",
        "            src_mask = src_mask.unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def create_trg_mask(self, trg):\n",
        "        if self.efficient_mha:\n",
        "            trg_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "            mask = torch.ones((1, self.num_heads, trg.shape[1], trg.shape[1])).triu(1).to(self.device)\n",
        "        else:\n",
        "            trg_mask = (trg != self.trg_pad_idx).unsqueeze(1)\n",
        "            mask = torch.ones((1, trg.shape[1], trg.shape[1])).triu(1).to(self.device)\n",
        "        mask = mask == 0\n",
        "        trg_mask = trg_mask & mask\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.create_src_mask(src)\n",
        "        trg_mask = self.create_trg_mask(trg)\n",
        "        encoder_outputs, encoder_mha_attn_weights = self.encoder(src, src_mask)\n",
        "        decoder_outputs, _, enc_dec_mha_attn_weights = self.decoder(trg, encoder_outputs, trg_mask, src_mask)\n",
        "        logits = self.linear_layer(decoder_outputs)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "rbPT8ygAa1oT"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}