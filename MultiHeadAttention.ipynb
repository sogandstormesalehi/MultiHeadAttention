{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ksQfHimuiW0C"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Optional, List\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformKQV(nn.Module):\n",
        "\n",
        "  def __init__(self, model_dim, heads, vec_dim_per_head, bias):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(model_dim, heads * vec_dim_per_head, bias = bias)\n",
        "    self.heads = heads\n",
        "    self.vec_dim_per_head = vec_dim_per_head\n",
        "\n",
        "  def forward(self, x):\n",
        "    head = x.shape[:-1]\n",
        "    x = self.linear(x)\n",
        "    x = x.view(*head, self.heads, self.vec_dim_per_head)\n",
        "    return x"
      ],
      "metadata": {
        "id": "cCBLaTtEi83p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, heads, num_features_kqv, dropout = 0.1, bias = True):\n",
        "    super().__init__()\n",
        "    self.num_features_head = num_features_kqv // heads\n",
        "    self.heads = heads\n",
        "    self.query = TransformKQV(num_features_kqv, heads, self.num_features_head, bias = bias)\n",
        "    self.key = TransformKQV(num_features_kqv, heads, self.num_features_head, bias = bias)\n",
        "    self.value = TransformKQV(num_features_kqv, heads, self.num_features_head, bias = True)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.output = nn.Linear(num_features_kqv, num_features_kqv)\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "    self.scale = 1 / math.sqrt(self.num_features_kqv)\n",
        "    self.attention = None\n",
        "  \n",
        "  def get_scores(self, query, key):\n",
        "    return torch.einsum('ibhd, jbhd -> ijbh', query, key)\n",
        "\n",
        "  def mask_gen(self, mask, query_shape, key_shape):\n",
        "    assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
        "    assert mask.shape[1] == key_shape[0]\n",
        "    assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
        "    mask = mask.unsqueeze(-1)\n",
        "    return mask\n",
        "\n",
        "  def forward(self, *, query, key, value, mask = None):\n",
        "    sequence_length, batch_size, _ = query.shape\n",
        "    if mask is not None:\n",
        "      mask = self.prepare_mask(mask, query.shape, key.shape)\n",
        "    query = self.query(query)\n",
        "    key = self.key(key)\n",
        "    value = self.value(value)\n",
        "    scores = self.get_scores(query, key)\n",
        "    scores *= self.scale\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    attention = self.softmax(scores)\n",
        "    attention = self.dropout(attention)\n",
        "    x = torch.einsum(\"ijbh,jbhd->ibhd\", attention, value)\n",
        "    self.attention = attention.detach()\n",
        "    x = x.reshape(sequence_length, batch_size, -1)\n",
        "    return self.output(x)"
      ],
      "metadata": {
        "id": "Ncd7eS7wk3he"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}